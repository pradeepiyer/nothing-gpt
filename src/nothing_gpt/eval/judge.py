"""LLM-as-judge scoring via OpenAI API."""

import json
import re
from dataclasses import asdict, dataclass
from pathlib import Path

from openai import OpenAI

from nothing_gpt.eval.generate import GeneratedResponse


@dataclass
class JudgeScores:
    character: str
    generation_character: str
    character_consistency: int
    humor: int
    coherence: int
    overall: int
    reasoning: str


@dataclass
class JudgeMetrics:
    mean_character_consistency: float
    mean_humor: float
    mean_coherence: float
    mean_overall: float
    per_character: dict[str, dict[str, float]]


SCORE_FIELDS = ("character_consistency", "humor", "coherence", "overall")


def build_judge_prompt(
    character: str,
    system_prompt: str,
    context_messages: list[dict[str, str]],
    generated_response: str,
) -> str:
    """Build a structured rubric prompt for LLM-as-judge scoring."""
    context_text = ""
    for msg in context_messages:
        role_label = "User" if msg["role"] == "user" else "Assistant"
        context_text += f"{role_label}: {msg['content']}\n"

    rubric = (
        "1. character_consistency: Does the response sound like "
        f"{character}? Does it match their personality, "
        "speech patterns, and typical concerns?\n"
        "2. humor: Is the response funny in a way that fits "
        "Seinfeld's style? Does it use observational comedy, "
        "absurdity, or character-driven humor?\n"
        "3. coherence: Does the response make sense given the "
        "conversation context? Is it a natural continuation?\n"
        "4. overall: Overall quality as a Seinfeld character response."
    )
    json_format = (
        '{"character_consistency": <1-5>, "humor": <1-5>, '
        '"coherence": <1-5>, "overall": <1-5>, '
        '"reasoning": "<brief explanation>"}'
    )

    return f"""You are evaluating a response generated by a Seinfeld character chatbot.

CHARACTER: {character}
CHARACTER DESCRIPTION: {system_prompt}

CONVERSATION CONTEXT:
{context_text}
GENERATED RESPONSE: {generated_response}

Rate the generated response on each dimension from 1 (worst) to 5 (best):

{rubric}

Respond with JSON only, in this exact format:
{json_format}"""


@dataclass
class ParsedScores:
    character_consistency: int
    humor: int
    coherence: int
    overall: int
    reasoning: str


def parse_judge_response(response_text: str) -> ParsedScores:
    """Parse the judge's JSON response, handling markdown fences.

    Returns parsed scores with character_consistency, humor, coherence, overall (int 1-5)
    and reasoning (str). Raises ValueError on invalid input.
    """
    # Strip markdown code fences if present
    text = response_text.strip()
    match = re.search(r"```(?:json)?\s*(.*?)\s*```", text, re.DOTALL)
    if match:
        text = match.group(1)

    try:
        data = json.loads(text)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in judge response: {e}") from e

    for field in SCORE_FIELDS:
        if field not in data:
            raise ValueError(f"Missing required field: {field}")
        score = data[field]
        if not isinstance(score, int) or score < 1 or score > 5:
            raise ValueError(f"Score for {field} must be an integer 1-5, got {score}")

    if "reasoning" not in data:
        raise ValueError("Missing required field: reasoning")

    return ParsedScores(
        character_consistency=data["character_consistency"],
        humor=data["humor"],
        coherence=data["coherence"],
        overall=data["overall"],
        reasoning=data["reasoning"],
    )


def judge_responses(
    responses: list[GeneratedResponse],
    max_responses: int = 0,
    model: str = "gpt-4o-mini",
) -> list[JudgeScores]:
    """Score responses using OpenAI as judge. Expects OPENAI_API_KEY env var."""
    client = OpenAI(timeout=120)
    to_judge = responses[:max_responses] if max_responses > 0 else responses
    scores: list[JudgeScores] = []

    for resp in to_judge:
        prompt = build_judge_prompt(
            character=resp.character,
            system_prompt=resp.system_prompt,
            context_messages=resp.context,
            generated_response=resp.generated,
        )

        completion = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        raw = completion.choices[0].message.content or ""

        parsed = parse_judge_response(raw)
        scores.append(JudgeScores(
            character=resp.character,
            generation_character=resp.generation_character,
            character_consistency=parsed.character_consistency,
            humor=parsed.humor,
            coherence=parsed.coherence,
            overall=parsed.overall,
            reasoning=parsed.reasoning,
        ))

    return scores


def compute_metrics(scores: list[JudgeScores]) -> JudgeMetrics:
    """Compute mean scores overall and per character."""
    if not scores:
        return JudgeMetrics(
            mean_character_consistency=0,
            mean_humor=0,
            mean_coherence=0,
            mean_overall=0,
            per_character={},
        )

    totals = {f: 0.0 for f in SCORE_FIELDS}
    per_char: dict[str, dict[str, list[int]]] = {}

    for s in scores:
        for f in SCORE_FIELDS:
            totals[f] += getattr(s, f)

        char_scores = per_char.setdefault(s.character, {f: [] for f in SCORE_FIELDS})
        for f in SCORE_FIELDS:
            char_scores[f].append(getattr(s, f))

    n = len(scores)
    per_char_means: dict[str, dict[str, float]] = {}
    for char, field_lists in sorted(per_char.items()):
        per_char_means[char] = {
            f: sum(vals) / len(vals) for f, vals in field_lists.items()
        }

    return JudgeMetrics(
        mean_character_consistency=totals["character_consistency"] / n,
        mean_humor=totals["humor"] / n,
        mean_coherence=totals["coherence"] / n,
        mean_overall=totals["overall"] / n,
        per_character=per_char_means,
    )


def save_scores(scores: list[JudgeScores], path: Path) -> None:
    """Save judge scores to JSONL."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w") as f:
        for s in scores:
            f.write(json.dumps(asdict(s)) + "\n")


def load_scores(path: Path) -> list[JudgeScores]:
    """Load judge scores from JSONL."""
    scores: list[JudgeScores] = []
    with open(path) as f:
        for line in f:
            data = json.loads(line)
            scores.append(JudgeScores(**data))
    return scores
